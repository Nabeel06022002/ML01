{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1HYm+shtNjaHSeFYUcaSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nabeel06022002/ML01/blob/main/ML01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYIGIKawzry4"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os as os\n",
        "import os.path as osp\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib as mpl\n",
        "mpl.use('Agg')# AGG(Anti-Grain Geometry engine)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision import models,transforms\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "from itertools import product\n",
        "from math import sqrt\n",
        "import time\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import hashlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding:utf-8\n",
        "\n",
        "\n",
        "class PhaseShuffle(nn.Module):\n",
        "\t#Definition of layers to perform phaseshuffle\n",
        "\tdef __init__(self,n):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.n = n#The range of how to shift is defined as [-n, n] in the paper.\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t#If N is 0, it is equivalent to phaseshuffle in the first place\n",
        "\t\tif self.n == 0:\n",
        "\t\t\treturn x\n",
        "\t\t#The integer belonging to [-n, n] is randomly generated and shift\n",
        "\t\tshift = torch.Tensor(x.shape[0]).random_(-self.n,self.n+1).type(torch.int)\n",
        "\t\t#Store the result of applying phaseshuffle to X in x_shuffled and return it as a return value.\n",
        "\t\tx_shuffled = x.clone()\n",
        "\t\tfor i,shift_num in enumerate(shift):\n",
        "\t\t\tif(shift_num==0): continue\n",
        "\t\t\tdim = len(x_shuffled[i].size()) - 1\n",
        "\t\t\torigin_length = x[i].shape[dim]\n",
        "\t\t\tif shift_num > 0:\n",
        "\t\t\t\tleft = torch.flip(torch.narrow(x[i],dim,1,shift_num),[dim])\n",
        "\t\t\t\tright = torch.narrow(x[i],dim,0,origin_length-shift_num)\n",
        "\t\t\telse:\n",
        "\t\t\t\tshift_num = -shift_num\n",
        "\t\t\t\tleft = torch.narrow(x[i],dim,shift_num,origin_length-shift_num)\n",
        "\t\t\t\tright = torch.flip(torch.narrow(x[i],dim,origin_length-shift_num-1,shift_num),[dim])\n",
        "\t\t\tx_shuffled[i] = torch.cat([left,right],dim)\n",
        "\n",
        "\t\treturn x_shuffled\n",
        "\n",
        "#Functions that require the \"Gradient_penalty\" function required for calculating the gradient constraints of Discripor\n",
        "#In WGAN-GP, the loss function of DISRIMINATOR is represented as E [Judgment Results of Real Voice] -E [Judgment Results of False Vehical Vehicle]+gradient constraints.\n",
        "#In Generator, it is described as E [Judgment Results of False Voice]\n",
        "def gradient_penalty(netD,real,fake,batch_size,gamma=1):\n",
        "\tdevice = real.device\n",
        "\t#For Tensor where Requires_grad is valid, the backward method can be called and can automatically calculate the differentiation.\n",
        "\talpha = torch.rand(batch_size,1,1,requires_grad=True).to(device)\n",
        "\t#Mix the real and fake at a random ratio\n",
        "\tx = alpha*real + (1-alpha)*fake\n",
        "\t#Put it in Discriminator and make the result D_\n",
        "\td_ = netD.forward(x)\n",
        "\t#Output D_ and input x\n",
        "\t#It is known that if the L2 norm calculated from the inclination becomes 1, it will produce good results.\n",
        "\t#Therefore, calculate Gradient_penalty so that this can be learned so that this approaches 1\n",
        "\tg = torch.autograd.grad(outputs=d_, inputs=x,\n",
        "\t\t\t\t\t\t\tgrad_outputs=torch.ones(d_.shape).to(device),\n",
        "\t\t\t\t\t\t\tcreate_graph=True, retain_graph=True,only_inputs=True)[0]\n",
        "\tg = g.reshape(batch_size, -1)\n",
        "\treturn ((g.norm(2,dim=1)/gamma-1.0)**2).mean()"
      ],
      "metadata": {
        "id": "oIG2SfrB0ZHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding:utf-8\n",
        "\n",
        "\n",
        "def make_datapath_list(target_path):\n",
        "\t#Read the dataset\n",
        "\tpath_list = []#Create a list of dataset file paths and return\n",
        "\tfor path in glob.glob(target_path,recursive=True):\n",
        "\t\tpath_list.append(path)\n",
        "\t\t##If you need to display all reading paths, remove the comment out\n",
        "\t\t#print(path)\n",
        "\t#Display the number of audio data to be read\n",
        "\tprint(\"sounds : \" + str(len(path_list)))\n",
        "\treturn path_list\n",
        "\n",
        "class GAN_Sound_Dataset(data.Dataset):\n",
        "\t#Voice dataset class\n",
        "\tdef __init__(self,file_list,device,batch_size,sound_length=65536,sampling_rate=16000,dat_threshold=1100):\n",
        "\t\t#file_list     : List of voice paths to read\n",
        "\t\t#device        : Decide whether to process with GPU\n",
        "\t\t#batch_size    : Batch size\n",
        "\t\t#sound_length  : Length of sound used for learning\n",
        "\t\t#sampling_rate : Sampling rate when reading audio\n",
        "\t\t#dat_threshold : If the total number of files in the dataset is below Dat_threshold, hold the file content\n",
        "\t\tself.file_list = file_list\n",
        "\t\tself.device = device\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.sound_length = sound_length\n",
        "\t\tself.sampling_rate = sampling_rate\n",
        "\t\tself.dat_threshold = dat_threshold\n",
        "\t\t#If the total number of files in the dataset is below Dat_threshold, hold the file content\n",
        "\t\tif(len(self.file_list)<=dat_threshold):\n",
        "\t\t\tself.file_contents = []\n",
        "\t\t\tfor file_path in self.file_list:\n",
        "\t\t\t\t#Sound is Numpy.ndarray, and the data of the chronological sound is stored.\n",
        "\t\t\t\tsound,_ = librosa.load(file_path,sr=self.sampling_rate)\n",
        "\t\t\t\tself.file_contents.append(sound)\n",
        "\n",
        "\t#Returns the larger batch size and the total number of files\n",
        "\tdef __len__(self):\n",
        "\t\treturn max(self.batch_size, len(self.file_list))\n",
        "\t#Get data in Tensor format with pre -processed audio\n",
        "\tdef __getitem__(self,index):\n",
        "\t\tif(len(self.file_list)<=self.dat_threshold):\n",
        "\t\t\tsound = self.file_contents[index%len(self.file_list)]\n",
        "\t\telse:\n",
        "\t\t\t#Take out one from the list of paths\n",
        "\t\t\tsound_path = self.file_list[index%len(self.file_list)]\n",
        "\t\t\t#Sound is Numpy.ndarray, and the data of the chronological sound is stored.\n",
        "\t\t\tsound,_ = librosa.load(sound_path,sr=self.sampling_rate)\n",
        "\t\t#Convert to Tensor format\n",
        "\t\tsound = (torch.from_numpy(sound.astype(np.float32)).clone()).to(self.device)\n",
        "\t\t#If there is an element that is larger than 1 in the time series sound data, it is normalized so that it will be 1.\n",
        "\t\tmax_amplitude = torch.max(torch.abs(sound))\n",
        "\t\tif max_amplitude > 1:\n",
        "\t\t\tsound /= max_amplitude\n",
        "\t\t#Make the length of the loaded sound as LOADED_SOUND_LENGTH\n",
        "\t\tloaded_sound_length = sound.shape[0]\n",
        "\t\t#If the length of the loaded sound is below Sound_length,\n",
        "\t\t#Fill the front and rear of the sound by 0 and align the length to Self.sound_length\n",
        "\t\tif loaded_sound_length < self.sound_length:\n",
        "\t\t\tpadding_length = self.sound_length - loaded_sound_length\n",
        "\t\t\tleft_zeros = torch.zeros(padding_length//2).to(self.device)\n",
        "\t\t\tright_zeros = torch.zeros(padding_length - padding_length//2).to(self.device)\n",
        "\t\t\tsound = torch.cat([left_zeros,sound,right_zeros],dim=0).to(self.device)\n",
        "\t\t\tloaded_sound_length = self.sound_length\n",
        "\t\t#Choose a random part from the readable sound for the length of the sound used for learning and cut it out.\n",
        "\t\tif loaded_sound_length > self.sound_length:\n",
        "\t\t\t#Select the starting point randomly\n",
        "\t\t\tstart_index = torch.randint(0,(loaded_sound_length-self.sound_length)//2,(1,1))[0][0].item()\n",
        "\t\t\tend_index = start_index + self.sound_length\n",
        "\t\t\tsound = sound[start_index:end_index]\n",
        "\t\t#At this point, Sound.shape is TORCH.SIZE ([3, Self.Sound_length]),\n",
        "\t\t#Convert this to torch.size ([3, 1, self.sound_length])\n",
        "\t\tsound = sound.unsqueeze(0)\n",
        "\t\treturn sound\n",
        "\n",
        "#Produced voice output function\n",
        "def save_sounds(path,sounds,sampling_rate):\n",
        "\tnow_time = time.time()\n",
        "\tfor i,sound in enumerate(sounds):\n",
        "\t\tsound = sound.squeeze(0)\n",
        "\t\tsound = sound.to('cpu').detach().numpy().copy()\n",
        "\t\thash_string = hashlib.md5(str(now_time).encode()).hexdigest()\n",
        "\t\tfile_path = os.path.join(path,f\"generated_sound_{i}_{hash_string}.wav\")\n",
        "\t\tprint(file_path)\n",
        "\t\tsf.write(file_path,sound,sampling_rate,format=\"WAV\")\n",
        "\n",
        "#Operation confirmation\n",
        "# train_wav_list = make_datapath_list('../dataset/**/*.wav')\n",
        "\n",
        "# batch_size = 3\n",
        "# dataset = GAN_Sound_Dataset(file_list=train_wav_list,device=\"cpu\",batch_size=batch_size)\n",
        "\n",
        "# dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "# batch_iterator = iter(dataloader)\n",
        "# sounds = next(batch_iterator)\n",
        "# save_sounds(sounds,16000)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5KUS04mB0i8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding:utf-8\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\tdef __init__(self,model_size=32,z_dim=20):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.model_size = model_size #The value that is described as D in the dissertation\n",
        "\n",
        "\t\tself.full_connection_1 = nn.Linear(z_dim,512*model_size)\n",
        "\n",
        "\t\tself.layer_1 = nn.Sequential(\n",
        "\t\t\t\tnn.ConvTranspose1d(in_channels=32*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tout_channels=16*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tkernel_size=25,\\\n",
        "\t\t\t\t\t\t\t\t\tstride=4,\\\n",
        "\t\t\t\t\t\t\t\t\tpadding=11,\\\n",
        "\t\t\t\t\t\t\t\t\toutput_padding=1),\n",
        "\t\t\t\tnn.ReLU(inplace=True))\n",
        "\t\tself.layer_2 = nn.Sequential(\n",
        "\t\t\t\tnn.ConvTranspose1d(in_channels=16*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tout_channels=8*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tkernel_size=25,\\\n",
        "\t\t\t\t\t\t\t\t\tstride=4,\\\n",
        "\t\t\t\t\t\t\t\t\tpadding=11,\\\n",
        "\t\t\t\t\t\t\t\t\toutput_padding=1),\n",
        "\t\t\t\tnn.ReLU(inplace=True))\n",
        "\t\tself.layer_3 = nn.Sequential(\n",
        "\t\t\t\tnn.ConvTranspose1d(in_channels=  8*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tout_channels=4*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tkernel_size=25,\\\n",
        "\t\t\t\t\t\t\t\t\tstride=4,\\\n",
        "\t\t\t\t\t\t\t\t\tpadding=11,\\\n",
        "\t\t\t\t\t\t\t\t\toutput_padding=1),\n",
        "\t\t\t\tnn.ReLU(inplace=True))\n",
        "\t\tself.layer_4 = nn.Sequential(\n",
        "\t\t\t\tnn.ConvTranspose1d(in_channels=  4*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tout_channels=2*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tkernel_size=25,\\\n",
        "\t\t\t\t\t\t\t\t\tstride=4,\\\n",
        "\t\t\t\t\t\t\t\t\tpadding=11,\\\n",
        "\t\t\t\t\t\t\t\t\toutput_padding=1),\n",
        "\t\t\t\tnn.ReLU(inplace=True))\n",
        "\t\tself.layer_5 = nn.Sequential(\n",
        "\t\t\t\tnn.ConvTranspose1d(in_channels=  2*model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tout_channels=  model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tkernel_size=25,\\\n",
        "\t\t\t\t\t\t\t\t\tstride=4,\\\n",
        "\t\t\t\t\t\t\t\t\tpadding=11,\\\n",
        "\t\t\t\t\t\t\t\t\toutput_padding=1),\n",
        "\t\t\t\tnn.ReLU(inplace=True))\n",
        "\t\tself.layer_6 = nn.Sequential(\n",
        "\t\t\t\tnn.ConvTranspose1d(in_channels=model_size,\\\n",
        "\t\t\t\t\t\t\t\t\tout_channels=1,\\\n",
        "\t\t\t\t\t\t\t\t\tkernel_size=25,\\\n",
        "\t\t\t\t\t\t\t\t\tstride=4,\\\n",
        "\t\t\t\t\t\t\t\t\tpadding=11,\\\n",
        "\t\t\t\t\t\t\t\t\toutput_padding=1),\n",
        "\t\t\t\tnn.Tanh())\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.full_connection_1(x).view(-1,32*self.model_size,16)\n",
        "\t\tx = F.relu(x)\n",
        "\t\tx = self.layer_1(x)\n",
        "\t\tx = self.layer_2(x)\n",
        "\t\tx = self.layer_3(x)\n",
        "\t\tx = self.layer_4(x)\n",
        "\t\tx = self.layer_5(x)\n",
        "\t\toutput = self.layer_6(x)\n",
        "\t\treturn output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H3JXmAc10wL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding:utf-8\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "\tdef __init__(self,model_size=32,shift_factor=2):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.model_size = model_size #The value that is described as D in the dissertation\n",
        "\t\tself.shift_factor = shift_factor  #n How to shake\n",
        "\n",
        "\t\tself.layer_1 = nn.Sequential(\n",
        "\t\t\t\tnn.Conv1d(           1,   model_size,kernel_size=25,stride=4,padding=11),\n",
        "\t\t\t\tnn.LeakyReLU(0.2,inplace=True),\n",
        "\t\t\t\tPhaseShuffle(shift_factor)\n",
        "\t\t\t\t)\n",
        "\t\tself.layer_2 = nn.Sequential(\n",
        "\t\t\t\tnn.Conv1d(  model_size, 2*model_size,kernel_size=25,stride=4,padding=11),\n",
        "\t\t\t\tnn.LeakyReLU(0.2,inplace=True),\n",
        "\t\t\t\tPhaseShuffle(shift_factor)\n",
        "\t\t\t\t)\n",
        "\t\tself.layer_3 = nn.Sequential(\n",
        "\t\t\t\tnn.Conv1d(2*model_size, 4*model_size,kernel_size=25,stride=4,padding=11),\n",
        "\t\t\t\tnn.LeakyReLU(0.2,inplace=True),\n",
        "\t\t\t\tPhaseShuffle(shift_factor)\n",
        "\t\t\t\t)\n",
        "\t\tself.layer_4 = nn.Sequential(\n",
        "\t\t\t\tnn.Conv1d(4*model_size, 8*model_size,kernel_size=25,stride=4,padding=11),\n",
        "\t\t\t\tnn.LeakyReLU(0.2,inplace=True),\n",
        "\t\t\t\tPhaseShuffle(shift_factor)\n",
        "\t\t\t\t)\n",
        "\t\tself.layer_5 = nn.Sequential(\n",
        "\t\t\t\tnn.Conv1d(8*model_size,16*model_size,kernel_size=25,stride=4,padding=11),\n",
        "\t\t\t\tnn.LeakyReLU(0.2,inplace=True),\n",
        "\t\t\t\tPhaseShuffle(shift_factor)\n",
        "\t\t\t\t)\n",
        "\t\tself.layer_6 = nn.Sequential(\n",
        "\t\t\t\tnn.Conv1d(16*model_size,32*model_size,kernel_size=25,stride=4,padding=11),\n",
        "\t\t\t\tnn.LeakyReLU(0.2,inplace=True),\n",
        "\t\t\t\tPhaseShuffle(shift_factor)\n",
        "\t\t\t\t)\n",
        "\n",
        "\t\tself.full_connection_1 = nn.Linear(512*model_size,1)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.layer_1(x)\n",
        "\t\tx = self.layer_2(x)\n",
        "\t\tx = self.layer_3(x)\n",
        "\t\tx = self.layer_4(x)\n",
        "\t\tx = self.layer_5(x)\n",
        "\t\tx = self.layer_6(x)\n",
        "\t\tx = x.view(-1,512*self.model_size)\n",
        "\t\toutput = self.full_connection_1(x)\n",
        "\t\treturn output\n"
      ],
      "metadata": {
        "id": "KbQ0vuhY05lZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding:utf-8\n",
        "\n",
        "\n",
        "#Formatting the path to each data to make_dataPath_list for datasets\n",
        "dataset_path = '/content/dataset/SM1_F1_A01.wav'\n",
        "#Batch size\n",
        "batch_size = 16\n",
        "#The size of the random number to enter\n",
        "z_dim = 20\n",
        "#Number of epochs\n",
        "num_epochs = 500\n",
        "#Learning rate used for Optimizer\n",
        "lr = 0.0001\n",
        "#Input and output sound sampling rate\n",
        "sampling_rate = 16000\n",
        "#How many times to learn Discripor per study at GENERATOR\n",
        "D_updates_per_G_update = 5\n",
        "#Generate_sounds_interval [Epoch] Outputs the learning status every time you learn\n",
        "generate_sounds_interval = 20\n",
        "\n",
        "#Check if GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\",device)\n",
        "\n",
        "#Read training data, create dataset\n",
        "train_sound_list = make_datapath_list(dataset_path)\n",
        "train_dataset = GAN_Sound_Dataset(file_list=train_sound_list,device=device,batch_size=batch_size)\n",
        "#generator用\n",
        "dataloader_for_G = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "#discriminator用\n",
        "dataloader_for_D = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "# #Functions for initializing networks\n",
        "def weights_init(m):\n",
        "\tif isinstance(m, nn.Conv1d) or isinstance(m, nn.ConvTranspose1d) or isinstance(m,nn.Linear):\n",
        "\t\tnn.init.kaiming_normal_(m.weight.data)\n",
        "\n",
        "#Generate Generator instance\n",
        "netG = Generator(z_dim=z_dim)\n",
        "#Move the network to the device\n",
        "netG = netG.to(device)\n",
        "#Initialization of network\n",
        "netG.apply(weights_init)\n",
        "\n",
        "#Generate DISCRIMINATOR instance\n",
        "netD = Discriminator()\n",
        "#Move the network to the device\n",
        "netD = netD.to(device)\n",
        "#Initialization of network\n",
        "netD.apply(weights_init)\n",
        "\n",
        "#Set the optimization method to ADAM\n",
        "beta1 = 0.5\n",
        "beta2 = 0.9\n",
        "optimizerD = optim.Adam(netD.parameters(),lr=lr,betas=(beta1,beta2))\n",
        "optimizerG = optim.Adam(netG.parameters(),lr=lr,betas=(beta1,beta2))\n",
        "\n",
        "#Start of learning\n",
        "#Variables to follow the learning process\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "#Noise to enter Generator to follow the learning process\n",
        "generating_num = 5#How many sounds do you want to output?\n",
        "z_sample = torch.Tensor(generating_num,z_dim).uniform_(-1,1).to(device)\n",
        "\n",
        "print(\"Starting Training\")\n",
        "\n",
        "#Save the learning start time\n",
        "t_epoch_start = time.time()\n",
        "#Loop for each epoch\n",
        "for epoch in range(num_epochs):\n",
        "\t#BATCH_SIZE Take out and learn from the dataset\n",
        "\tfor generator_i,real_sound_for_G in enumerate(dataloader_for_G, 0):\n",
        "\t\t#-------------------------\n",
        " \t\t#Learning of DISCRIMINATOR\n",
        "\t\t#-------------------------\n",
        "\t\t#Loss function e [Real Voice Judgment Results] -E [Judgment Results of False Voice]+Learn to maximize the gradient constraints of the gradient\n",
        "\t\t#Learn D_Updates_per_g_Update times Discriminator per study of Generator\n",
        " \t\t#-------------------------\n",
        "\t\terrD_loss_sum = 0#Variables for taking the average of losses during learning\n",
        "\t\tfor discriminator_i,real_sound_for_D in enumerate(dataloader_for_D, 0):\n",
        "\t\t\tif(discriminator_i==D_updates_per_G_update): break\n",
        "\t\t\t#Number of audio data actually taken out\n",
        "\t\t\tminibatch_size = real_sound_for_D.shape[0]\n",
        "\t\t\t#If the number of mini batches taken out is 1, it will be an error in the process of finding the gradient, so skip the processing.\n",
        "\t\t\tif(minibatch_size==1): continue\n",
        "\t\t\t#If you can use GPU, transfer to GPU\n",
        "\t\t\treal_sound_for_D = real_sound_for_D.to(device)\n",
        "\t\t\t#Generate noise and make Z\n",
        "\t\t\tz = torch.Tensor(minibatch_size,z_dim).uniform_(-1,1).to(device)\n",
        "\t\t\t#Put noise in GENERATOR and generate fake sounds and make Fake_sound.\n",
        "\t\t\tfake_sound = netG.forward(z)\n",
        "\t\t\t#Judge the real sound and store the results in D\n",
        "\t\t\td_real = netD.forward(real_sound_for_D)\n",
        "\t\t\t#Judge the false sound and store the result in D_\n",
        "\t\t\td_fake = netD.forward(fake_sound)\n",
        "\n",
        "\t\t\t#Take the average of the judgment results for each mini batch\n",
        "\t\t\tloss_real = d_real.mean()#-E. Calculate [Real Voice Judgment Results]\n",
        "\t\t\tloss_fake = d_fake.mean()#-E. Calculate [Judgment Results of False Vehicle]\n",
        "\t\t\t#Calculation of gradient constraints\n",
        "\t\t\tloss_gp = gradient_penalty(netD,real_sound_for_D.data,fake_sound.data,minibatch_size)\n",
        "\t\t\tbeta_gp = 10.0\n",
        "\t\t\t#E[Real audio judgment result] -E [Judgment result of false audio]+gradient constraints calculation\n",
        "\t\t\terrD = -loss_real + loss_fake + beta_gp*loss_gp\n",
        "\t\t\t#The inclination calculated in the previous itelation has remained, so reset it.\n",
        "\t\t\toptimizerD.zero_grad()\n",
        "\t\t\t#Calculate the inclination of the loss\n",
        "\t\t\terrD.backward()\n",
        "\t\t\t#Actually propagate errors\n",
        "\t\t\toptimizerD.step()\n",
        "\t\t\t#Record Loss to take the average later\n",
        "\t\t\terrD_loss_sum += errD.item()\n",
        "\t\t\n",
        "\t\t#-------------------------\n",
        " \t\t#Generator learning\n",
        "\t\t#-------------------------\n",
        "\t\t#Loss function -E Learn to maximize [Judgment Results of False Voice]\n",
        " \t\t#-------------------------\n",
        "\t\t#Number of audio data actually taken out\n",
        "\t\tminibatch_size = real_sound_for_G.shape[0]\n",
        "\t\t#If the number of mini batches taken out is 1, it will be an error in the process of finding the gradient, so skip the processing.\n",
        "\t\tif(minibatch_size==1): continue\n",
        "\t\t#If you can use GPU, transfer to GPU\n",
        "\t\treal_sound_for_G = real_sound_for_G.to(device)\n",
        "\t\t#Generate noise\n",
        "\t\tz = torch.Tensor(minibatch_size,z_dim).uniform_(-1,1).to(device)\n",
        "\t\t#Enter the noise into the Generator and make the output audio as Fake_sound.\n",
        "\t\tfake_sound = netG.forward(z)\n",
        "\t\t#Output audio fake_sound is inferred or fake sound in Discriminator\n",
        "\t\td_fake = netD.forward(fake_sound)\n",
        "\n",
        "\t\t# WGAN_GP takes an average for all inference results in the mini batch and use it for erroneous propagation.\n",
        "\t\terrG = -d_fake.mean()#E Calculate the [Judgment Results of False Voice]\n",
        "\t\t#The inclination calculated in the previous itelation has remained, so reset it.\n",
        "\t\toptimizerG.zero_grad()\n",
        "\t\t#Calculate the inclination of the loss\n",
        "\t\terrG.backward()\n",
        "\t\t#Actually propagate errors\n",
        "\t\toptimizerG.step()\n",
        "\n",
        "\t\t#Record Loss to output to the graph later\n",
        "\t\tG_losses.append(errG.item())\n",
        "\t\tD_losses.append(errD_loss_sum/D_updates_per_G_update)\n",
        "\n",
        "\t\titers += 1\n",
        "\t\t#Break for testing\n",
        "\t\t#break\n",
        "\t\n",
        "\t#Output the learning status\n",
        "\tif (epoch%generate_sounds_interval==0 or epoch==num_epochs-1):\n",
        "\t\tprint('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\t'\n",
        "\t\t\t\t% (epoch, num_epochs,\n",
        "\t\t\t\t\terrD_loss_sum/D_updates_per_G_update, errG.item()))\n",
        "\t\t#Create if there is no output directory\n",
        "\t\toutput_dir = \"./output/train/generated_epoch_{}\".format(epoch)\n",
        "\t\tif not os.path.exists(output_dir):\n",
        "\t\t\tos.makedirs(output_dir)\n",
        "\t\t#Output of generated audio\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tgenerated_sound = netG(z_sample)\n",
        "\t\t\tsave_sounds(output_dir,generated_sound,sampling_rate)\n",
        "\n",
        "#-------------------------\n",
        "#Output of execution result\n",
        "#-------------------------\n",
        "\n",
        "#Output the time spent on learning\n",
        "#Record the time at the end of learning\n",
        "t_epoch_finish = time.time()\n",
        "total_time = t_epoch_finish - t_epoch_start\n",
        "with open('./output/train/time.txt', mode='w') as f:\n",
        "\tf.write(\"total_time: {:.4f} sec.\\n\".format(total_time))\n",
        "\tf.write(\"dataset size: {}\\n\".format(len(train_sound_list)))\n",
        "\tf.write(\"num_epochs: {}\\n\".format(num_epochs))\n",
        "\tf.write(\"batch_size: {}\\n\".format(batch_size))\n",
        "\n",
        "#Output a learned Generator model (for CPU)\n",
        "torch.save(netG.to('cpu').state_dict(),\"./output/generator_trained_model_cpu.pth\")\n",
        "\n",
        "#Output Loss graph\n",
        "plt.clf()\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig('./output/train/loss.png')\n",
        "\n",
        "print(\"data generated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KHp4iCX1AP5",
        "outputId": "e73c28a6-9316-4426-8750-a22ddf5e3755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n",
            "sounds : 1\n",
            "Starting Training\n",
            "[0/500]\tLoss_D: 0.4501\tLoss_G: 10.6582\t\n",
            "./output/train/generated_epoch_0/generated_sound_0_b0439f866b0e8fbbc8f079f7dabebf2e.wav\n",
            "./output/train/generated_epoch_0/generated_sound_1_b0439f866b0e8fbbc8f079f7dabebf2e.wav\n",
            "./output/train/generated_epoch_0/generated_sound_2_b0439f866b0e8fbbc8f079f7dabebf2e.wav\n",
            "./output/train/generated_epoch_0/generated_sound_3_b0439f866b0e8fbbc8f079f7dabebf2e.wav\n",
            "./output/train/generated_epoch_0/generated_sound_4_b0439f866b0e8fbbc8f079f7dabebf2e.wav\n",
            "[20/500]\tLoss_D: -1.3978\tLoss_G: 10.9386\t\n",
            "./output/train/generated_epoch_20/generated_sound_0_b7432aeea3499001e922753c16a631d9.wav\n",
            "./output/train/generated_epoch_20/generated_sound_1_b7432aeea3499001e922753c16a631d9.wav\n",
            "./output/train/generated_epoch_20/generated_sound_2_b7432aeea3499001e922753c16a631d9.wav\n",
            "./output/train/generated_epoch_20/generated_sound_3_b7432aeea3499001e922753c16a631d9.wav\n",
            "./output/train/generated_epoch_20/generated_sound_4_b7432aeea3499001e922753c16a631d9.wav\n",
            "[40/500]\tLoss_D: -1.6541\tLoss_G: 2.9417\t\n",
            "./output/train/generated_epoch_40/generated_sound_0_d94403ebba24664d4259b0e43d388de4.wav\n",
            "./output/train/generated_epoch_40/generated_sound_1_d94403ebba24664d4259b0e43d388de4.wav\n",
            "./output/train/generated_epoch_40/generated_sound_2_d94403ebba24664d4259b0e43d388de4.wav\n",
            "./output/train/generated_epoch_40/generated_sound_3_d94403ebba24664d4259b0e43d388de4.wav\n",
            "./output/train/generated_epoch_40/generated_sound_4_d94403ebba24664d4259b0e43d388de4.wav\n",
            "[60/500]\tLoss_D: -0.8202\tLoss_G: 9.6021\t\n",
            "./output/train/generated_epoch_60/generated_sound_0_6d78962f0e2a41175d02c5af79fe2aa4.wav\n",
            "./output/train/generated_epoch_60/generated_sound_1_6d78962f0e2a41175d02c5af79fe2aa4.wav\n",
            "./output/train/generated_epoch_60/generated_sound_2_6d78962f0e2a41175d02c5af79fe2aa4.wav\n",
            "./output/train/generated_epoch_60/generated_sound_3_6d78962f0e2a41175d02c5af79fe2aa4.wav\n",
            "./output/train/generated_epoch_60/generated_sound_4_6d78962f0e2a41175d02c5af79fe2aa4.wav\n",
            "[80/500]\tLoss_D: -1.4420\tLoss_G: -13.5645\t\n",
            "./output/train/generated_epoch_80/generated_sound_0_51456723dfe92e5e08fd6cae9ac3a4f8.wav\n",
            "./output/train/generated_epoch_80/generated_sound_1_51456723dfe92e5e08fd6cae9ac3a4f8.wav\n",
            "./output/train/generated_epoch_80/generated_sound_2_51456723dfe92e5e08fd6cae9ac3a4f8.wav\n",
            "./output/train/generated_epoch_80/generated_sound_3_51456723dfe92e5e08fd6cae9ac3a4f8.wav\n",
            "./output/train/generated_epoch_80/generated_sound_4_51456723dfe92e5e08fd6cae9ac3a4f8.wav\n",
            "[100/500]\tLoss_D: -0.7959\tLoss_G: 6.8876\t\n",
            "./output/train/generated_epoch_100/generated_sound_0_206b968eb30ed3f57f38bbf8024aa294.wav\n",
            "./output/train/generated_epoch_100/generated_sound_1_206b968eb30ed3f57f38bbf8024aa294.wav\n",
            "./output/train/generated_epoch_100/generated_sound_2_206b968eb30ed3f57f38bbf8024aa294.wav\n",
            "./output/train/generated_epoch_100/generated_sound_3_206b968eb30ed3f57f38bbf8024aa294.wav\n",
            "./output/train/generated_epoch_100/generated_sound_4_206b968eb30ed3f57f38bbf8024aa294.wav\n",
            "[120/500]\tLoss_D: -1.1739\tLoss_G: 4.2934\t\n",
            "./output/train/generated_epoch_120/generated_sound_0_0b564033e9ceddcd15056d94011671d5.wav\n",
            "./output/train/generated_epoch_120/generated_sound_1_0b564033e9ceddcd15056d94011671d5.wav\n",
            "./output/train/generated_epoch_120/generated_sound_2_0b564033e9ceddcd15056d94011671d5.wav\n",
            "./output/train/generated_epoch_120/generated_sound_3_0b564033e9ceddcd15056d94011671d5.wav\n",
            "./output/train/generated_epoch_120/generated_sound_4_0b564033e9ceddcd15056d94011671d5.wav\n",
            "[140/500]\tLoss_D: -1.6336\tLoss_G: 9.5552\t\n",
            "./output/train/generated_epoch_140/generated_sound_0_12d0452f95da5636e8eadc9df7e9ed1f.wav\n",
            "./output/train/generated_epoch_140/generated_sound_1_12d0452f95da5636e8eadc9df7e9ed1f.wav\n",
            "./output/train/generated_epoch_140/generated_sound_2_12d0452f95da5636e8eadc9df7e9ed1f.wav\n",
            "./output/train/generated_epoch_140/generated_sound_3_12d0452f95da5636e8eadc9df7e9ed1f.wav\n",
            "./output/train/generated_epoch_140/generated_sound_4_12d0452f95da5636e8eadc9df7e9ed1f.wav\n",
            "[160/500]\tLoss_D: -1.6311\tLoss_G: 1.0201\t\n",
            "./output/train/generated_epoch_160/generated_sound_0_2e72c57f448fc621f9b568703ae0a4b2.wav\n",
            "./output/train/generated_epoch_160/generated_sound_1_2e72c57f448fc621f9b568703ae0a4b2.wav\n",
            "./output/train/generated_epoch_160/generated_sound_2_2e72c57f448fc621f9b568703ae0a4b2.wav\n",
            "./output/train/generated_epoch_160/generated_sound_3_2e72c57f448fc621f9b568703ae0a4b2.wav\n",
            "./output/train/generated_epoch_160/generated_sound_4_2e72c57f448fc621f9b568703ae0a4b2.wav\n",
            "[180/500]\tLoss_D: -1.3950\tLoss_G: 2.5669\t\n",
            "./output/train/generated_epoch_180/generated_sound_0_09908e085d5eee9c64d0f7d3ed8d1408.wav\n",
            "./output/train/generated_epoch_180/generated_sound_1_09908e085d5eee9c64d0f7d3ed8d1408.wav\n",
            "./output/train/generated_epoch_180/generated_sound_2_09908e085d5eee9c64d0f7d3ed8d1408.wav\n",
            "./output/train/generated_epoch_180/generated_sound_3_09908e085d5eee9c64d0f7d3ed8d1408.wav\n",
            "./output/train/generated_epoch_180/generated_sound_4_09908e085d5eee9c64d0f7d3ed8d1408.wav\n",
            "[200/500]\tLoss_D: -1.4399\tLoss_G: 14.0080\t\n",
            "./output/train/generated_epoch_200/generated_sound_0_81778e32eea078d74703e2d9040aecb8.wav\n",
            "./output/train/generated_epoch_200/generated_sound_1_81778e32eea078d74703e2d9040aecb8.wav\n",
            "./output/train/generated_epoch_200/generated_sound_2_81778e32eea078d74703e2d9040aecb8.wav\n",
            "./output/train/generated_epoch_200/generated_sound_3_81778e32eea078d74703e2d9040aecb8.wav\n",
            "./output/train/generated_epoch_200/generated_sound_4_81778e32eea078d74703e2d9040aecb8.wav\n",
            "[220/500]\tLoss_D: -1.8108\tLoss_G: 5.2307\t\n",
            "./output/train/generated_epoch_220/generated_sound_0_dc9686f35071c9f91256397b95a915a5.wav\n",
            "./output/train/generated_epoch_220/generated_sound_1_dc9686f35071c9f91256397b95a915a5.wav\n",
            "./output/train/generated_epoch_220/generated_sound_2_dc9686f35071c9f91256397b95a915a5.wav\n",
            "./output/train/generated_epoch_220/generated_sound_3_dc9686f35071c9f91256397b95a915a5.wav\n",
            "./output/train/generated_epoch_220/generated_sound_4_dc9686f35071c9f91256397b95a915a5.wav\n",
            "[240/500]\tLoss_D: -0.7884\tLoss_G: 17.3664\t\n",
            "./output/train/generated_epoch_240/generated_sound_0_305051fc177bff41a9f1e1101eefd4ff.wav\n",
            "./output/train/generated_epoch_240/generated_sound_1_305051fc177bff41a9f1e1101eefd4ff.wav\n",
            "./output/train/generated_epoch_240/generated_sound_2_305051fc177bff41a9f1e1101eefd4ff.wav\n",
            "./output/train/generated_epoch_240/generated_sound_3_305051fc177bff41a9f1e1101eefd4ff.wav\n",
            "./output/train/generated_epoch_240/generated_sound_4_305051fc177bff41a9f1e1101eefd4ff.wav\n",
            "[260/500]\tLoss_D: 0.0772\tLoss_G: 1.6028\t\n",
            "./output/train/generated_epoch_260/generated_sound_0_26913af91e6234e1b321b877e3e0cb4a.wav\n",
            "./output/train/generated_epoch_260/generated_sound_1_26913af91e6234e1b321b877e3e0cb4a.wav\n",
            "./output/train/generated_epoch_260/generated_sound_2_26913af91e6234e1b321b877e3e0cb4a.wav\n",
            "./output/train/generated_epoch_260/generated_sound_3_26913af91e6234e1b321b877e3e0cb4a.wav\n",
            "./output/train/generated_epoch_260/generated_sound_4_26913af91e6234e1b321b877e3e0cb4a.wav\n",
            "[280/500]\tLoss_D: -0.2553\tLoss_G: 17.2072\t\n",
            "./output/train/generated_epoch_280/generated_sound_0_6f8f2fba087f7eebe306576f30797d41.wav\n",
            "./output/train/generated_epoch_280/generated_sound_1_6f8f2fba087f7eebe306576f30797d41.wav\n",
            "./output/train/generated_epoch_280/generated_sound_2_6f8f2fba087f7eebe306576f30797d41.wav\n",
            "./output/train/generated_epoch_280/generated_sound_3_6f8f2fba087f7eebe306576f30797d41.wav\n",
            "./output/train/generated_epoch_280/generated_sound_4_6f8f2fba087f7eebe306576f30797d41.wav\n",
            "[300/500]\tLoss_D: 1.3421\tLoss_G: 12.3962\t\n",
            "./output/train/generated_epoch_300/generated_sound_0_88e949a4574436fea71c4dd5eec404eb.wav\n",
            "./output/train/generated_epoch_300/generated_sound_1_88e949a4574436fea71c4dd5eec404eb.wav\n",
            "./output/train/generated_epoch_300/generated_sound_2_88e949a4574436fea71c4dd5eec404eb.wav\n",
            "./output/train/generated_epoch_300/generated_sound_3_88e949a4574436fea71c4dd5eec404eb.wav\n",
            "./output/train/generated_epoch_300/generated_sound_4_88e949a4574436fea71c4dd5eec404eb.wav\n",
            "[320/500]\tLoss_D: -0.2030\tLoss_G: 3.2433\t\n",
            "./output/train/generated_epoch_320/generated_sound_0_4c6276a635e1fc0b4f8b0ace4ee027d1.wav\n",
            "./output/train/generated_epoch_320/generated_sound_1_4c6276a635e1fc0b4f8b0ace4ee027d1.wav\n",
            "./output/train/generated_epoch_320/generated_sound_2_4c6276a635e1fc0b4f8b0ace4ee027d1.wav\n",
            "./output/train/generated_epoch_320/generated_sound_3_4c6276a635e1fc0b4f8b0ace4ee027d1.wav\n",
            "./output/train/generated_epoch_320/generated_sound_4_4c6276a635e1fc0b4f8b0ace4ee027d1.wav\n",
            "[340/500]\tLoss_D: -0.2670\tLoss_G: 7.5531\t\n",
            "./output/train/generated_epoch_340/generated_sound_0_dd5ad62deb869e0fdcd1d1d76d5de83c.wav\n",
            "./output/train/generated_epoch_340/generated_sound_1_dd5ad62deb869e0fdcd1d1d76d5de83c.wav\n",
            "./output/train/generated_epoch_340/generated_sound_2_dd5ad62deb869e0fdcd1d1d76d5de83c.wav\n",
            "./output/train/generated_epoch_340/generated_sound_3_dd5ad62deb869e0fdcd1d1d76d5de83c.wav\n",
            "./output/train/generated_epoch_340/generated_sound_4_dd5ad62deb869e0fdcd1d1d76d5de83c.wav\n",
            "[360/500]\tLoss_D: -0.4812\tLoss_G: 10.1854\t\n",
            "./output/train/generated_epoch_360/generated_sound_0_261ac9d96e7e8e87396f072da78a7ccf.wav\n",
            "./output/train/generated_epoch_360/generated_sound_1_261ac9d96e7e8e87396f072da78a7ccf.wav\n",
            "./output/train/generated_epoch_360/generated_sound_2_261ac9d96e7e8e87396f072da78a7ccf.wav\n",
            "./output/train/generated_epoch_360/generated_sound_3_261ac9d96e7e8e87396f072da78a7ccf.wav\n",
            "./output/train/generated_epoch_360/generated_sound_4_261ac9d96e7e8e87396f072da78a7ccf.wav\n",
            "[380/500]\tLoss_D: -0.3458\tLoss_G: 11.0224\t\n",
            "./output/train/generated_epoch_380/generated_sound_0_98feb36cb46d144f8ddb0fbc63ed2579.wav\n",
            "./output/train/generated_epoch_380/generated_sound_1_98feb36cb46d144f8ddb0fbc63ed2579.wav\n",
            "./output/train/generated_epoch_380/generated_sound_2_98feb36cb46d144f8ddb0fbc63ed2579.wav\n",
            "./output/train/generated_epoch_380/generated_sound_3_98feb36cb46d144f8ddb0fbc63ed2579.wav\n",
            "./output/train/generated_epoch_380/generated_sound_4_98feb36cb46d144f8ddb0fbc63ed2579.wav\n",
            "[400/500]\tLoss_D: -0.7000\tLoss_G: 7.1311\t\n",
            "./output/train/generated_epoch_400/generated_sound_0_e4965c048f0f9a99ec63e4b67f46d472.wav\n",
            "./output/train/generated_epoch_400/generated_sound_1_e4965c048f0f9a99ec63e4b67f46d472.wav\n",
            "./output/train/generated_epoch_400/generated_sound_2_e4965c048f0f9a99ec63e4b67f46d472.wav\n",
            "./output/train/generated_epoch_400/generated_sound_3_e4965c048f0f9a99ec63e4b67f46d472.wav\n",
            "./output/train/generated_epoch_400/generated_sound_4_e4965c048f0f9a99ec63e4b67f46d472.wav\n",
            "[420/500]\tLoss_D: -0.2361\tLoss_G: 5.0750\t\n",
            "./output/train/generated_epoch_420/generated_sound_0_59af38c297ba598728d508b075f25f65.wav\n",
            "./output/train/generated_epoch_420/generated_sound_1_59af38c297ba598728d508b075f25f65.wav\n",
            "./output/train/generated_epoch_420/generated_sound_2_59af38c297ba598728d508b075f25f65.wav\n",
            "./output/train/generated_epoch_420/generated_sound_3_59af38c297ba598728d508b075f25f65.wav\n",
            "./output/train/generated_epoch_420/generated_sound_4_59af38c297ba598728d508b075f25f65.wav\n",
            "[440/500]\tLoss_D: -1.1448\tLoss_G: 19.8858\t\n",
            "./output/train/generated_epoch_440/generated_sound_0_72de334941e602ac5b9b9d6a5d545d34.wav\n",
            "./output/train/generated_epoch_440/generated_sound_1_72de334941e602ac5b9b9d6a5d545d34.wav\n",
            "./output/train/generated_epoch_440/generated_sound_2_72de334941e602ac5b9b9d6a5d545d34.wav\n",
            "./output/train/generated_epoch_440/generated_sound_3_72de334941e602ac5b9b9d6a5d545d34.wav\n",
            "./output/train/generated_epoch_440/generated_sound_4_72de334941e602ac5b9b9d6a5d545d34.wav\n",
            "[460/500]\tLoss_D: -0.6965\tLoss_G: 8.8260\t\n",
            "./output/train/generated_epoch_460/generated_sound_0_2c8c1d44c403dcf2fcc3213b23d16f15.wav\n",
            "./output/train/generated_epoch_460/generated_sound_1_2c8c1d44c403dcf2fcc3213b23d16f15.wav\n",
            "./output/train/generated_epoch_460/generated_sound_2_2c8c1d44c403dcf2fcc3213b23d16f15.wav\n",
            "./output/train/generated_epoch_460/generated_sound_3_2c8c1d44c403dcf2fcc3213b23d16f15.wav\n",
            "./output/train/generated_epoch_460/generated_sound_4_2c8c1d44c403dcf2fcc3213b23d16f15.wav\n",
            "[480/500]\tLoss_D: -1.0196\tLoss_G: 9.0092\t\n",
            "./output/train/generated_epoch_480/generated_sound_0_eeed970b76f56a92c50ed0ded7957594.wav\n",
            "./output/train/generated_epoch_480/generated_sound_1_eeed970b76f56a92c50ed0ded7957594.wav\n",
            "./output/train/generated_epoch_480/generated_sound_2_eeed970b76f56a92c50ed0ded7957594.wav\n",
            "./output/train/generated_epoch_480/generated_sound_3_eeed970b76f56a92c50ed0ded7957594.wav\n",
            "./output/train/generated_epoch_480/generated_sound_4_eeed970b76f56a92c50ed0ded7957594.wav\n",
            "[499/500]\tLoss_D: -0.8574\tLoss_G: -0.4579\t\n",
            "./output/train/generated_epoch_499/generated_sound_0_d45ac16b5bee16010447f0149b7509d9.wav\n",
            "./output/train/generated_epoch_499/generated_sound_1_d45ac16b5bee16010447f0149b7509d9.wav\n",
            "./output/train/generated_epoch_499/generated_sound_2_d45ac16b5bee16010447f0149b7509d9.wav\n",
            "./output/train/generated_epoch_499/generated_sound_3_d45ac16b5bee16010447f0149b7509d9.wav\n",
            "./output/train/generated_epoch_499/generated_sound_4_d45ac16b5bee16010447f0149b7509d9.wav\n",
            "data generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding:utf-8\n",
        "\n",
        "\n",
        "#Number of audio files to be output\n",
        "sample_size = 16\n",
        "#The size of the random number to enter\n",
        "z_dim = 20\n",
        "#Sampling rate of voice to handle\n",
        "sampling_rate = 16000\n",
        "\n",
        "#Read the learned model\n",
        "netG = Generator(z_dim=z_dim)\n",
        "trained_model_path = \"./output/generator_trained_model_cpu.pth\"\n",
        "netG.load_state_dict(torch.load(trained_model_path))\n",
        "#Switch to inference mode\n",
        "netG.eval()\n",
        "#Noise generation\n",
        "noise = torch.Tensor(sample_size,z_dim).uniform_(-1,1)\n",
        "#Enter GENERATOR and get output image\n",
        "generated_sound = netG(noise)\n",
        "#Create if there is no output directory\n",
        "output_dir = \"./output/inference\"\n",
        "if not os.path.exists(output_dir):\n",
        "\tos.makedirs(output_dir)\n",
        "#Output of audio file\n",
        "save_sounds(\"./output/inference/\",generated_sound,sampling_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfVFqLho2GqD",
        "outputId": "0a0ade40-6966-4370-ecbb-a31b98671f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./output/inference/generated_sound_0_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_1_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_2_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_3_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_4_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_5_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_6_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_7_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_8_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_9_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_10_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_11_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_12_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_13_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_14_127b4c17d3a5606aa9f5f448f761c528.wav\n",
            "./output/inference/generated_sound_15_127b4c17d3a5606aa9f5f448f761c528.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WCPjFDzx4FzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}